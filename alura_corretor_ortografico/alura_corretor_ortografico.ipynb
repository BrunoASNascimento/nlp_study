{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('study': conda)",
   "display_name": "Python 3.8.5 64-bit ('study': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9af793d265c01b97965a8e494342f25e4cd356d588870941bfd4deefcc594e2c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a\nª\nà\ná\nã\na-amilase\ná-bê-cê\ná-é-i-ó-u\na-histórico\nà-propos\nà-toa\na.a.\nA.C.\na.C.\na.m.\naa.\naabora\nAAC\n\n"
     ]
    }
   ],
   "source": [
    "with open('data/palavras_ptbr.txt','r',encoding='utf8') as file:\n",
    "    articles = file.read()\n",
    "print(articles[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_splited = nltk.tokenize.word_tokenize(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(list_token):\n",
    "    list_words=[]\n",
    "    for token in list_token:\n",
    "        if token.isalpha():\n",
    "            list_words.append(token)\n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of word is 271122.\n"
     ]
    }
   ],
   "source": [
    "list_words = split_words(words_splited)\n",
    "print(f'The number of word is {len(list_words)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(list_words):\n",
    "    normalized_list=[word.lower() for word in list_words]\n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of word is 269680.\n"
     ]
    }
   ],
   "source": [
    "normalized_list = set(normalization(list_words))\n",
    "print(f'The number of word is {len(normalized_list)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_word = 'lgica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letters(slices):\n",
    "    new_words =[]\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
    "    for l,r in slices:\n",
    "        for letter in letters:\n",
    "            new_words.append(''.join([l,letter,r])) \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['algica', 'blgica', 'clgica', 'dlgica', 'elgica']"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "words_created = create_words(example_word)\n",
    "words_created[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_character(slices):\n",
    "    new_words =[]    \n",
    "    for l,r in slices:\n",
    "        if len(r)>1:\n",
    "            new_words.append(''.join([l,r[1],r[0]+r[2:]])) \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_letters(slices):\n",
    "    new_words =[]\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
    "    for l,r in slices:\n",
    "        for letter in letters:\n",
    "            new_words.append(''.join([l,letter,r[1:]])) \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverter_caracter(slices):\n",
    "    new_words =[]    \n",
    "    for l,r in slices:\n",
    "        new_words.append(''.join([l,r[1:]])) \n",
    "    return new_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_words(word):\n",
    "    slices=[]\n",
    "    for i in range(len(word)+1):\n",
    "        slices.append((word[:i],word[i:]))\n",
    "    words_created = insert_letters(slices)\n",
    "    words_created +=remove_character(slices)\n",
    "    words_created +=change_letters(slices)\n",
    "    return words_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrector(word):\n",
    "    words_created = create_words(word)\n",
    "    word_correct = max(words_created,key=probability)\n",
    "    return word_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = nltk.FreqDist(normalized_list)\n",
    "\n",
    "def probability(word):\n",
    "    return frequency[word]/len(normalized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_test(name_file):\n",
    "    list_words =[]\n",
    "    f = open(name_file,'r',encoding='utf8')\n",
    "    for row in f:\n",
    "        correct, wrong =row.split()\n",
    "        list_words.append((correct, wrong))\n",
    "    f.close()\n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('podemos', 'pyodemos'),\n",
       " ('esse', 'esje'),\n",
       " ('já', 'jrá'),\n",
       " ('nosso', 'nossov'),\n",
       " ('são', 'sãêo')]"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "list_test = create_data_test('data/palavras.txt')\n",
    "list_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(tests,vocabulary):\n",
    "    number_words=len(tests)\n",
    "    correct = 0\n",
    "    unknown =0\n",
    "    for  correct_word, wrong_word in tests:\n",
    "        corrected_word = corrector(wrong_word)\n",
    "        if corrected_word==correct_word:\n",
    "            correct+=1\n",
    "        else:\n",
    "            unknown+=(correct_word not in vocabulary)\n",
    "    hit_rate = correct/number_words\n",
    "    unknown_rate = unknown/number_words\n",
    "    print(f'Hit rate: {round(hit_rate,4)*100}%')\n",
    "    print(f'Unknown rate: {round(unknown_rate,4)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hit rate: 22.58%\nUnknown rate: 16.669999999999998%\n"
     ]
    }
   ],
   "source": [
    "evaluator(list_test,normalized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_words_turbo(words):\n",
    "    new_words =[]\n",
    "    for word in words:\n",
    "        new_words += create_words(word)\n",
    "    return new_words"
   ]
  }
 ]
}